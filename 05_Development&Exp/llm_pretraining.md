# llm pretraining

[torchtitan](https://arxiv.org/abs/2410.06511) is a production ready framework for llm pretraining which allow four different parallelism (data, tensor, pipeline and sequence). it also supports hardware-software co-design features such as Float8 training and SymmetricMemory.


GRU (Gated Recurrent Unit) is a type of RNN which designed for efficient sequential data modeling. It is widely used for temporal or sequential tasks due to its ability to capture dependencies over time with fewer parameters and less computational cost compared to LSTMs.


